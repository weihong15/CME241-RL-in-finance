{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP is defined by\n",
    "\n",
    "* a set of states $s\\in S$\n",
    "* a set of actions $a\\in A$\n",
    "* action dependent **transition probabilities** $T(s,a,s') = P(s'|s,a)$ so that for each state s and action a $\\sum_{s'\\in S} T(s,a,s') = 1$\n",
    "* Reward functions $R(s,a,s')$ : reward for starting in state s, taking action a and ending up in state s'\n",
    "\n",
    "## Utility function\n",
    "We will use discounted reward system\n",
    "\\begin{equation}\n",
    "U[s_0,s_1,\\ldots ]= \\sum _{k=0}^{\\infty } \\gamma ^ k R(s_ k) \\leq \\frac{R_{max}}{1-\\gamma}.\n",
    "\\end{equation}\n",
    "\n",
    "## Policy and Value Function\n",
    "* **Policy** $\\pi^*:s \\rightarrow a$, best action u can take given a state, to maximize expected utility $argmax E(U(s))$\n",
    "* **Value** $V^*(s)$ expected reward if the agently acts optimally starting at state s. $\\max E(U(s))$\n",
    "\n",
    "## Bellman Equation\n",
    "* **Q-equation** $Q^*(s,a)$ - expected reward starting at state s, taking action a and acting optimally afterwards  \n",
    "Bellman equation, \n",
    "\\begin{align}\n",
    "V^*(s) & = \\max_a Q^*(s,a) = Q^*(s,\\pi^*(s)) \\\\\n",
    "Q^*(s,a) & =\\sum_{s'} T(s,a,s')(R(s,a,s')+\\gamma V^*(s')) \\\\\n",
    "V^*(s) & = \\max_a \\sum_{s'} T(s,a,s')(R(s,a,s')+\\gamma V^*(s'))\n",
    "\\end{align}\n",
    "\n",
    "## Value Iteration Algorithm\n",
    "Define $V_k^*(s)$ as expected reward from state s after k steps, note as $k \\rightarrow \\infty, V_k^*(s) \\rightarrow V(s)$  \n",
    "* Initialize $V_0^*(s) = 0$\n",
    "* Iterate til $V_k^*(s) \\approx V_{k+1}^*(s)$ for all s \n",
    "    * $V_{k+1}^*(s) = \\max_a \\sum_{s'} T(s,a,s')(R(s,a,s')+\\gamma V_k^*(s'))$\n",
    "* once converge we compute the $Q^*(s,a)$ using bellman, and compute $\\pi^*(s) = argmax_a Q^*(s,a)$\n",
    "\n",
    "## Q-Value Iteration Algorithm\n",
    "\\begin{equation}\n",
    "Q_{k+1}^*(s, a) = \\sum _{s'} T(s, a, s')\\left(R(s, a, s') + \\gamma \\text {max}_{a'} Q_ k^*(s', a')\\right)\n",
    "\\end{equation}\n",
    "This can be derived from first and second of bellman equation\n",
    "\n",
    "## MDP in real world\n",
    "We define MDP with 4 values previously, <S,A,T,R>. but in the real world, we often know only S and A. We do not know T and R before excuting the action. we may need to \"explore\" to find out T and R but we do not know them before hand. So in RL we often just have <S,A> and we need to estimate T and R.  \n",
    "\n",
    "### Model Free approach.\n",
    "We will sample K points from P(X) and directly estimate the expectation of f(X) as follows:\n",
    "\\begin{equation}\n",
    "E[f(X)] \\approx \\frac{\\displaystyle \\sum _{i=1}^ K f(X_ i)}{K}\n",
    "\\end{equation}\n",
    "\n",
    "Note for model based approach(which we do not use), we estimate p(x) using the count and $E[f(X)] \\approx \\displaystyle \\sum \\hat{p(x)} f(x)$\n",
    "\n",
    "## Naive Q-value iteration for RL\n",
    "We will get many different sample, assume from start from state S, we perform k times action a. \n",
    "\\begin{align}\n",
    "sample_1 &: R(s,a,s_1') + \\gamma \\max_{a'}Q(s_1',a') \\\\\n",
    "\\dots \\\\\n",
    "sample_k &: R(s,a,s_k') + \\gamma \\max_{a'}Q(s_k',a') \\\\\n",
    "Q(s,a) & = \\frac{1}{k} \\sum_{i=1}^k sample_i = \\frac{1}{k} \\sum_{i=1}^k (R(s,a,s_i') + \\alpha \\max_{a'} Q(s_i',a'))\n",
    "\\end{align}\n",
    "\n",
    "### Exponential running average instead of average\n",
    "\\begin{equation}\n",
    "\\bar{X}_n = \\frac{X_n + (1-\\alpha)X_{n-1} + (1-\\alpha)^2X_{n-2} + ...}{1+(1-\\alpha)+(1-\\alpha)^2 + ...}\n",
    "\\end{equation}\n",
    "Equivalently\n",
    "\\begin{equation}\n",
    "\\bar{X}_n = \\alpha X_n + (1-\\alpha)\\bar{X}_{n-1}\n",
    "\\end{equation}\n",
    "\n",
    "## New Q-value iteration for RL\n",
    "\\begin{align}\n",
    "Q_{i+1}(s,a) & = \\alpha \\cdot sample  + (1-\\alpha)Q_{i}(s,a) \\\\\n",
    "sample_1 &: R(s,a,s') + \\gamma \\max_{a'}Q_i(s',a') \\\\\n",
    "\\end{align}\n",
    "\n",
    "### Actual implementation\n",
    "1. Initialization: $Q(s,a) = 0, \\forall s,a$\n",
    "2. Iterate until convergence  \n",
    "    a. Collect samples $s,a,s',R(s,a,s')$  \n",
    "    b. $Q_{i+1}(s,a) = \\alpha \\cdot (R(s,a,s') + \\gamma \\max_{a'}Q_i(s',a'))  + (1-\\alpha)Q_{i}(s,a)$\n",
    "\n",
    "The 2b, is equivalent to $Q_{i+1}(s,a) = Q_{i}(s,a) + \\alpha(R(s,a,s') + \\gamma \\max_{a'}Q_i(s',a') - Q_{i}(s,a))$, which is similar to a stochastic descent method\n",
    "\n",
    "## Exploration vs Exploitation \n",
    "Exploration is about trying out actions that have been under examined and visiting states that were never visiited before or visited less often. Exploitation means taking the optimal action wrt current knowledge about environment. Making the best move(policy action).  \n",
    "\n",
    "### Epsilon greedy approach\n",
    "Randomly sample with probability $\\epsilon$ and choose best move(exploitation) with prob $1-\\epsilon$. It will Start with 1 and slowly decreases. Late game we tend to make the right move than explore.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "In this project, we address the task of learning control policies for text-based games using reinforcement learning. In these games, all interactions between players and the virtual world are through text. The current world state is described by elaborate text, and the underlying state is not directly observable. Players read descriptions of the state and respond with natural language commands to take actions.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
